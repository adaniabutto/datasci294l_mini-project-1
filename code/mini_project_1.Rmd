---
title: "DATSCI294L: Data Science and the Science of Learning"
subtitle: "Mini-Project #1: Predicting the diffi culty of a test item"
author: "Adani B. Abutto"
date: "`r format(Sys.Date(), '%B %d, %Y')`"

output:
  bookdown::html_document2:
    toc: true
    toc_depth: 3
    toc_float: true
    theme: cosmo
    highlight: tango

header-includes:
    - \usepackage{setspace}\doublespacing
---

# Setup

```{r, message = F, warning = F}

## load relevant libraries and functions
require(knitr)         # for knitting
library(Hmisc)         # for descriptives
library(png)           # for working with images
library(grid)
library(DT)            # for data tables
library(tidyverse)     # for everything else
library(ggeffects)     # for regression outputs
library(broom.mixed)
library(emmeans)
library(generics)
library(broom)
library(modelr)
library(lme4)          # for mixed effects models
library(ltm)           # for IRT stuff
library(TAM)
library(psych)

## set default code chunk options
knitr::opts_chunk$set(echo = T, warning = F, message = F)

## set default plot theme and colors
theme_set(theme_classic(base_size = 18))

## fix print width for knitted doc
options(width = 70)

## suppress warnings about grouping 
options(dplyr.summarise.inform = F)
options(xtable.floating = F)
options(xtable.timestamp = "")

## set random seed
set.seed(1)

## set directories for plots and data outputs
figures_dir = '../figures/'
data_dir = '../data/'

```

# Phase 1: Exploring and visualizing the data

## Load and examine the data

```{r}

# Import data as downloaded from https://data-visualization-benchmark.s3.us-west-2.amazonaws.com/vt-fusion/psych_139_mini_project_1_split_80_responses.csv

df.raw =
  read.csv(paste0(data_dir,
                  "psych_139_mini_project_1_split_80_responses.csv"))

# run quick data summary
skimr::skim(df.raw)

```

## Data Wrangling

```{r}

# convert data to wide format so each row represents one participant (N = 426)

df.wide =
  df.raw %>%
  # grab response values for each question from "correct_response" col
  pivot_wider(id_cols = "participant_id",
              names_from = c("question_text"),
              values_from = c("correct_response"),
              values_fn = mean)

# print top rows
head(df.wide, n = 10)

```

## Preliminary Item Difficulty Analysis: % correct

Across all tests, we have a total of 184 items (150 questions), with about 80-90 responses
per item.

```{r}

# Create summary df: For each question, compute % of participants who responded correctly (mean), SD, and SE

df.wide_summary =
  df.raw %>%
  group_by(question_text, test_name, graph_type, task_type_merged) %>%
  # compute mean
  dplyr::summarise(mean_correct = mean(correct_response,
                                       na.rm = T),
                   # compute SD
                   sd_correct = sd(correct_response,
                                   na.rm = T),
                   # compute number of responses
                   n = n(),
                   # compute SE (SD / sqrt of n)
                   se_correct = sd_correct/sqrt(n)) %>%
  ungroup()

# print top rows
head(df.wide_summary, n = 10)

```

### Viz 1: % correct for single items

The plot below shows that as expected, even within a single test, the items varied
considerably in their difficulty (as measured by % correct).

```{r fig.width = 20, fig.height = 10}

df.wide_summary %>%
  # question on x-axis, ordered by % correct
  ggplot(aes(x = reorder(question_text, mean_correct),
             # % correct on y-axis
             y = mean_correct*100),
         # color each bar based on graph type
         color = graph_type) +
  # add CIs based on 1.96*SE
  geom_errorbar(
    aes(ymin = (mean_correct - (1.96*se_correct))*100,
        ymax = (mean_correct + (1.96*se_correct))*100),
    width = 2,
    color = "black",
    alpha = .7,
    size = 1) +
  # plot one point per question/item
  geom_point(aes(fill = graph_type),
             shape = 21, color = "black", size = 2) +
  # facet by task type and test
  facet_wrap(~ task_type_merged + test_name,
             ncol = 5, scales = "free_y") +
  # adjust x-axis label orientation for readability
  theme(axis.text.x = element_text(angle = 45,
                                   hjust = 1,
                                   size = 1),
        legend.position = "none") +
  # fix scale to be 0-100, breaks of 25
  scale_y_continuous(limits = c(0, 100),
                     breaks = seq(0, 100, 25)) +
  # add title and axis labels
  labs(title = "Individual Item Difficulty by Task Type & Test",
       x = "Question",
       y = "% correct")

# export plot
ggsave((paste0(figures_dir,
               "pct_correct_tasktype_test.png")),
       width = 20, height = 10, device = "png")

```

### Viz 2: % correct averages by task type

The plot below shows that even though different tests contained items of the same "task
type", their respective item difficulty still varied across tests.

```{r}

# create new sub-dataframe: Within a given test, collapse across a given task type and compute % of participants who responded correctly (mean), SD, and SE
df.task_summary_tasktype =
  df.raw %>%
  # group by test and task type
  group_by(test_name, task_type_merged) %>%
  # compute mean
  summarise(mean_correct = mean(correct_response, na.rm = T),
            # compute SD
            sd_correct = sd(correct_response, na.rm = T),
            # compute number of responses
            n = n(),
            # compute SE
            se_correct = sd_correct / sqrt(n))

```

```{r fig.width = 10, fig.height = 6}

df.task_summary_tasktype %>%
  # test type on x-axis
  ggplot(aes(x = test_name,
             # % correct on y-axis
             y = (mean_correct*100),
             # color by test name
             fill = test_name,
             color = test_name)) +
  # add CIs based on 1.96*SE
  geom_errorbar(aes(ymin = (mean_correct - (1.96*(1.96*se_correct)))*100,
              ymax = (mean_correct + (1.96*se_correct))*100),
          position = position_dodge(width = 0.9),
          width = 0.1,
          color = "black") +
  # Add one point for each % correct collapsed across task type
  geom_point(position = position_dodge(width = 0.9),
             width = 1,
             size = 3) +
  # Adjust scales to be 0-100
  scale_y_continuous(limits = c(0, 100),
                     breaks = seq(0, 100, 25),
                     labels = scales::percent_format(scale = 1)) +
  # Add title and axis labels
  labs(title = "% correct averaged across Task Type",
       x = "Task Type",
       y = "% correct",
       fill = "Test") +
  # Adjust theme
  theme_minimal(base_size = 18) +
  theme(axis.text.x = element_blank(),
        axis.title.x = element_blank(),
        legend.position = "bottom") +
  guides(fill = "none") +
  # Facet by task type
  facet_grid(~task_type_merged)

# export plot
ggsave((paste0(figures_dir,
               "pct_correct_tasktype.png")),
       width = 10, height = 6, device = "png")

```

### Descriptives: Single-item descriptives for % correct

In the output below, we see that on some items, as few as 1% of participants selected the
correct response (very difficult items), and on other items, as many as 99% of
participants selected the correct response (very easy items).

Across items, SDs of % correct ranged from 10.6% to 50%.

```{r}

## Across dataset, compute means and SDs as well as mins and maxes for each respective statistic
df.wide_summary %>%
  dplyr::select(mean_correct, sd_correct) %>%
  summary()

# Print 10 questions with lowest average % correct
df.wide_summary %>%
  dplyr::select(question_text, mean_correct, sd_correct) %>%
  arrange(desc(mean_correct)) %>%
  tail(n = 10) %>%
  print()

# Print 10 questions with highest average % correct
df.wide_summary %>%
  dplyr::select(question_text, mean_correct, sd_correct) %>%
  arrange(desc(mean_correct)) %>%
  head(n = 10) %>%
  print()

# Print 10 questions with least variability
df.wide_summary %>%
  dplyr::select(question_text, mean_correct, sd_correct) %>%
  arrange(desc(sd_correct)) %>%
  tail(n = 10) %>%
  print()

# Print 10 questions with most variability
df.wide_summary %>%
  dplyr::select(question_text, mean_correct, sd_correct) %>%
  arrange(desc(sd_correct)) %>%
  head(n = 10) %>%
  print()

```

# Phase 2: Defining & evaluating statistical models

In the dataset we examined above, we have data from a series of tests that (supposedly)
tap data viz literacy. Notably, not all items in and across these tests are the same. In
any test, you want a variety of items. Based on this variety, itâ€™s likely some items will
be harder (and some will be easier) than others. I begin by performing some sanity checks
that assess whether the items we would expect to be harder were indeed harder for
participants (as per lower % correct).

## RQ1: Are items that were *designed to mislead* harder than those that were not?

That is, does misleading nature (categorical; yes vs. no) predict % correct?

**Note:** Only CALVI included misleading items. Thus, we examine data only from this test
(4,082 responses from *N* = 425 participants).

**Prediction:** Misleading items are lower % correct (i.e., higher difficulty) than
non-misleading items.

**Method:** Mixed effects logistic regression, with a fixed effect for item nature and a
random intercept per participant.

```{r}

# Extract relevant subset of data
df.misleading =
  df.raw %>%
  # Keep only relevant cols
  dplyr::select(participant_id, question_text, test_name, graph_type, task_type_merged, correct_response, misleading_item) %>%
  # Filter for CALVI data
  filter(test_name == "CALVI") %>%
  # Convert "misleading nature" into factor (True vs false)
  mutate(misleading_item = factor(misleading_item))

```

Out of the 48 items we have data on in CALVI, 36 are misleading (75%), and 12 are not
(25%).

```{r}

# count misleading items
df.misleading %>%
  distinct(question_text, misleading_item) %>%
  count(misleading_item)

```

### Descriptives: Mean and SD for misleading vs. non-misleading items

```{r}

df.misleading %>%
  group_by(misleading_item) %>%
  summarise(mean = mean(correct_response, na.rm = T),
            sd = sd(correct_response, na.rm = T),
            n = n())

```

### Viz 3: % correct for misleading vs non-misleading items

In line with the descriptive stats above, the plot shows that at first glance, the average
% correct for misleading items seems to be lower than the average % correct for
non-misleading items.

```{r fig.width = 16, fig.height = 8}

# Plot the raw data
df.misleading %>%
  # misleading nature (true vs false) on x-axis
  ggplot(aes(x = misleading_item,
             # % correct on y-axis
             y = (correct_response*100),
             # color by misleading nature
             fill = misleading_item)) +
  # add raw data points
  geom_point(position = position_jitter(width = .3,
                                        height = 0),
             shape = 21,
             size = 2,
             alpha = .3) +
  # add bootstrapped CIs for error bars
  stat_summary(fun.data = "mean_cl_boot",
               geom = "pointrange",
               color = "black",
               shape = 21,
               size = 1,
               linewidth = 3) +
  # adjuts scales
  scale_y_continuous(limits = c(0, 100),
                     breaks = seq(0, 100, 25),
                     labels = scales::percent_format(scale = 1)) +
  # add color scale for fill
  scale_fill_brewer(palette = "Pastel2") +
  # Adjust title and axis labels
  labs(title = "% correct by misleading vs. non-misleading items",
       x = "Misleading Item",
       y = "% correct",
       fill = "Misleading?") +
  # Adjust theme
  theme_classic(base_size = 36) +
  theme(legend.position = "none")

# export plot
ggsave((paste0(figures_dir,
               "pct_correct_misleading.png")),
       width = 16, height = 8, device = "png")
  
```

### Model A: Mixed Effects Logistic Regression: % correct \~ misleading

The logistic regression analysis shows that compared to a minimal model that fits the mean
regardless of item nature (misleading vs. not misleading), the model including item nature
improved fit to data:

$Ï‡^2$ = 356.6, df = 1, Î² = -1.51, SE = .080, *p* \< .001; *RMSE_model* = .463 vs
*RMSE_baseline **=*** 0*.*496.

Predicted % correct for non-misleading items was about twice as large (70%; 95% CIs =
[.67, .73]) as % correct for misleading items (34%; 95% CIs = [.32, .36]).

```{r}

df.misleading =
  df.misleading %>%
  mutate(participant_id = factor(participant_id))

# Run empty model fitting the mean
model.misleading_empty =
  glm(correct_response ~
          # intercept
          1,
      data = df.misleading)

model.misleading_empty %>%
  summary()

# RMSE
rmse_empty =
  # square root
  sqrt(
    # take mean
    mean((df.misleading$correct_response -
            predict(model.misleading_empty, type = "response"))^2
         # errors squared
         )
    )

print(rmse_empty)

########################

# Run augmented model adding predictor of interest (misleading nature)
model.misleading_augmented =
  glmer(correct_response ~
        # intercept
        1 +
        # fixed effect for item nature
        misleading_item +
        # random intercept for participant
        (1 | participant_id),
      data = df.misleading,
      family = "binomial")

# Print regression output
model.misleading_augmented %>%
  summary()

model.misleading_augmented %>%
  joint_tests()

# Show estimates of augmented model
model.misleading_augmented %>%
  ggpredict()

# Calculate model statistics
model.misleading_augmented %>% 
  broom.mixed::glance()

# Compute RMSE
rmse_misleading =
  # square root
  sqrt(
    # take mean
    mean((df.misleading$correct_response -
            predict(model.misleading_augmented, type = "response"))^2
         # errors squared
         )
    )

print(rmse_misleading)

# Compare empty model with augmented model
anova(model.misleading_empty, model.misleading_augmented)

```

### Model Visualizations

```{r fig.width = 16, fig.height = 8}

# plot the data with line for empty model prediction (mean)
df.misleading %>%
  # x-axis is misleading nature
  ggplot(aes(x = misleading_item,
             # y-axis is % correct
             y = (correct_response*100),
             # color by misleading nature
             fill = misleading_item)) +
  # add raw data points
  geom_point(position = position_jitter(width = .3,
                                        height = 0),
             shape = 21,
             size = 2,
             alpha = .3) +
  # add bootstrapped CIs
  stat_summary(fun.data = "mean_cl_boot",
               geom = "pointrange",
               color = "black",
               shape = 21,
               size = 1,
               linewidth = 3) +
  # add horizontal line for mean
  geom_hline(yintercept = mean(df.misleading$correct_response)*100,
             color = "lightblue", size = 3) +
  # adjust scale
  scale_y_continuous(limits = c(0, 100),
                     breaks = seq(0, 100, 25),
                     labels = scales::percent_format(scale = 1)) +
  # specify color fill
  scale_fill_brewer(palette = "Pastel2") +
  # add title and axis labels
  labs(title = "% correct by misleading vs. non-misleading items",
       x = "Misleading Item",
       y = "% correct",
       fill = "Misleading?") +
  # adjust theme
  theme_classic(base_size = 36) +
  theme(legend.position = "none")

```

```{r fig.width = 16, fig.height = 8}

# plot the data with lines for augmented model predictions (separate fitted means for misleading and non-misleading items)
df.misleading %>%
  # x-axis = misleading nature
  ggplot(aes(x = misleading_item,
             # y-axis = % correct
             y = (correct_response*100),
             # fill by misleading nature
             fill = misleading_item)) +
  # add raw data points
  geom_point(position = position_jitter(width = .3,
                                        height = 0),
             shape = 21,
             size = 2,
             alpha = .3) +
  # add boostrapped CIs
  stat_summary(fun.data = "mean_cl_boot",
               geom = "pointrange",
               color = "black",
               shape = 21,
               size = 1,
               linewidth = 3) +
  # add separate fitted lines for each mean
  geom_hline(yintercept = mean(df.misleading$correct_response[df.misleading$misleading_item == "True"])*100,
             color = "orange", size = 3) +
    geom_hline(yintercept = mean(df.misleading$correct_response[df.misleading$misleading_item == "False"])*100,
           color = "lightgreen", size = 3) +
  # adjust scale
  scale_y_continuous(limits = c(0, 100),
                     breaks = seq(0, 100, 25),
                     labels = scales::percent_format(scale = 1)) +
  # specify color fill
  scale_fill_brewer(palette = "Pastel2") +
  # add title and axis labels
  labs(title = "% correct by misleading vs. non-misleading items",
       x = "Misleading Item",
       y = "% correct",
       fill = "Misleading?") +
  # adjust  theme
  theme_classic(base_size = 36) +
  theme(legend.position = "none")

```

## RQ2: Are longer questions harder than shorter questions?

That is, does question length (continuous) predict % correct?

**Note:** I examined data from all tests (15,616 responses from *N* = 426 participants and
defined question length as the number of characters in the question.

**Prediction:** Longer questions are lower % correct (i.e., higher difficulty) than short
questions.

**Method:** Mixed effects linear regression, with a fixed effect for question length and a
random intercept per participant.

```{r}

# Extract relevant subset of data
df.length =
  df.raw %>%
  # Keep only relevant cols
  dplyr::select(participant_id, test_name, graph_type, task_type_merged, correct_response, question_text) %>%
  # Compute question length
  mutate(question_length = nchar(question_text))

# create summary df
df.length_summary =
  df.length %>%
  dplyr::select(question_text, correct_response) %>%
  group_by(question_text) %>%
  summarise(
    percent_correct = mean(correct_response, na.rm = TRUE) * 100,
    n = n(),
    question_length = nchar(first(question_text))
  ) %>%
  ungroup()

```

### Descriptives: Mean and SD of question length

There is some variation in question length both within and across tests, with the average
question length ranging from 48 to 95 characters.

```{r}

# create summary statistics for question length variable
df.length %>%
  group_by(test_name) %>%
  summarise(mean = mean(question_length),
            sd = sd(question_length))

```

### Viz 4: % correct by question length

The plot shows that at first glance, most questions are clustered around question length
50-150. Even within that range, % correct is highly variable, and there is not much of a
visually clear trend.

```{r}

# Plot raw data
df.length_summary %>%
  # x-axis = question length
  ggplot(aes(x = question_length,
             # y-axis = % correct
             y = percent_correct)) +
  # plot raw data
  geom_point(position = position_jitter(width = .5,
                                        height = 0),
             size = 2,
             alpha = .3) +
  # adjust scale
  scale_y_continuous(limits = c(0, 100),
                     breaks = seq(0, 100, 25),
                     labels = scales::percent_format(scale = 1)) +
  xlim(0, 375) +
  # add axis labels
  labs(y = "% correct",
       x = "Question Length")

```

### Model B: Mixed Effects Linear Regression Analysis: % correct \~ question length

The regression analysis shows that compared to a minimal model that fits the mean
regardless of question length, the model including question length improved fit to data:

$Ï‡^2$ = 430, df = 1, Î² standardized = -.37, SE = .0001, *p* \< .001; *RMSE_model* = .459
vs *RMSE_baseline **=*** 0*.*483.

On average, for every unit increase in question length (1 unit = 1 SD; standardized), %
correct was predicted to increase by .2%.

```{r}

df.length =
  df.length %>%
  mutate(participant_id = factor(participant_id),
         question_length = scale(question_length,
                                 center = T, scale = T))

# Run empty model fitting the mean
model.length_empty =
  glm(correct_response ~
        # intercept
        1,
      data = df.length)

model.length_empty %>%
  summary()

# RMSE
rmse_empty =
  # square root
  sqrt(
    # take mean
    mean((df.length$correct_response -
            predict(model.length_empty, type = "response"))^2
         # errors squared
         )
    )

print(rmse_empty)

########################

# Run augmented model adding predictor of interest (question length)
model.length_augmented =
  glmer(correct_response ~
          # intercept
          1 +
          # fixed effect for question length
          question_length +
          # random intercept for question
          (1 | participant_id),
        family = "binomial",
        data = df.length)

# Print regression output
model.length_augmented %>%
  summary()

model.length_augmented %>%
  joint_tests(pbkrtest.limit = 15616)

# Calculate model statistics
model.length_augmented %>% 
  broom.mixed::glance()

# Compute RMSE
rmse_questionlength =
  # square root
  sqrt(
    # take mean
    mean((df.length$correct_response -
            predict(model.length_augmented, type = "response"))^2
         # errors squared
         )
    )

print(rmse_questionlength)

# Compare empty model with augmented model
anova(model.length_empty, model.length_augmented)

```

### Model Visualizations

```{r}

# plot empty model with mean
df.length %>%
  # x-axis = question length
  ggplot(aes(x = question_length,
             # y-axis = % correct
             y = correct_response)) +
  # plot raw data points
  geom_point(position = position_jitter(width = .5,
                                        height = 0),
             size = 2,
             alpha = .5) +
  # add fitted line for mean
  geom_hline(yintercept = mean(df.length$correct_response),
             color = "lightblue", size = 1) +
  # adjust axis labels
  labs(y = "Correct (y = 1, n = 0)",
       x = "Question Length")

```

```{r}

# plot the data with a fitted line for the augmented model predictions
df.length %>%
  # question length on x-axis
  ggplot(aes(x = question_length,
             # % correct on y-axis
             y = correct_response)) +
  # add raw data
  geom_point(alpha = 0.6, size = 2) +
  # add regression line
  geom_smooth(method = "lm", se = TRUE, color = "lightblue") +
  # add title and axis labels
  labs(title = "% correct by question length",
       x = "Question Length",
       y = "Correct (y = 1, n = 0)")

```

## Comparing Model A and Model B

We have to reduce the data set used for model A to just CALVI items in order to match the
trial-level data that is fed into the model.

```{r}

# Create new data sets to match trial-level data fed into model
df.comparison_model =
  df.raw %>%
  # Keep only relevant cols
  dplyr::select(participant_id, question_text, test_name, graph_type, task_type_merged, correct_response, misleading_item) %>%
  # Filter for CALVI data
  filter(test_name == "CALVI") %>%
  # Convert "misleading nature" into factor (True vs false)
  mutate(misleading_item = factor(misleading_item)) %>%
  # calculate question length
  mutate(question_length = nchar(question_text)) %>%
  # format relevant variables
  mutate(participant_id = factor(participant_id),
       question_length = scale(question_length,
                               center = T, scale = T))

# Fit models anew

# Model A
model.comparison.misleading =
  glmer(correct_response ~
          # intercept
          1 +
          # fixed effect for question length
          misleading_item +
          # random intercept for question
          (1 | participant_id),
        family = "binomial",
        data = df.comparison_model)

# Model B
model.comparison.length =
  glmer(correct_response ~
          # intercept
          1 +
          # fixed effect for question length
          question_length +
          # random intercept for question
          (1 | participant_id),
        family = "binomial",
        data = df.comparison_model)

# Compare model A with model B
anova(model.comparison.misleading, model.comparison.length)

```

## RQ3: How informative and discriminative are the items in relation to data visualization literacy?

### Item Discrimination and Item Information

```{r}

# create new df
df.irt =
  df.raw

```

Next, we evaluate item information and item discrimination for all test items across all
tests from an IRT perspective.

In IRT, item difficulty is defined as the level of latent ability (i.e., data
visualization) or point on the ability scale (represented by theta) needed to have a 50%
probability of getting the item right. Harder items require more ability to have a 50%
probability of getting the item right. Easier items require less ability to have a 50%
probability of getting the item right.

We fit two-parameter logistic (2PL) models, where the expected difference in performance
(i.e., probability of getting the item right) depends not just on ability level but also
the specific item in question. In other words, subjects' responses are predicted by the
(hypothesized) ability as well as item difficulty.

**Item information** represents each item's ability to differentiate between test-takers
based on ability level (theta). We plot item information using Item Characteristic Curves
(ICCs), where the x-axis represents ability level (theta), and the y-axis represents
probability of getting a correct response. Higher item information is better.

**Item discrimination** represents how strongly related a given item is to the latent
ability in question as assessed by the broader test. Items with higher discrimination are
better at differentiating test-takers. More formally, it is the slope of a tangent at the
point of inflection of the item response function (IRF), or the point on the logistic
curve at which the curvature changes signs.

#### WAN

In the plots below, we see that for many items, respondents very low on data visualization
literacy still have a decent probability of choosing the correct answer. Items differ
quite widely in their discrimination.

```{r warning = F, message = F, results='hide'}

# Run for WAN
df.irt_wide = 
  df.irt %>%
  # Filter for WAN items
  filter(test_name == "WAN") %>%
  dplyr::select(participant_id, item_id, correct_response) %>%
  pivot_wider(names_from = item_id,
              values_from = correct_response) %>%
  dplyr::select(-c(participant_id))

# run 2pl model
model.2pl =
  # predict by difficulty + ability
  ltm(df.irt_wide ~ z1,
      # print IRT parameters (coefficient estimates of item difficulty and discrimination)
      IRT.param = TRUE,
      # 1000 iterations to yield estimates
      control = list(iter.em = 1000))

# item characteristic curves
plot(model.2pl,
     type = "ICC", zrange = c(-4, 4))

# item information plot
plot(model.2pl,
     type = "IIC", zrange = c(-4, 4))

# Wright map
fit_2pl_tam =
  tam.mml.2pl(df.irt_wide)

IRT.WrightMap(fit_2pl_tam)

```

#### GGR

In the plots below, we see that some items follow a step function. For some items, item
difficulty is high (even with high levels of ability, respondents have a low estimated
probability of selecting the correct answer. Again, items differ quite widely in their
discrimination.

```{r warning = F, message = F, results='hide'}

# Run for GGR
df.irt_wide = 
  df.irt %>%
  # Filter for GGR items
  filter(test_name == "GGR") %>%
  dplyr::select(participant_id, item_id, correct_response) %>%
  pivot_wider(names_from = item_id,
              values_from = correct_response) %>%
  dplyr::select(-c(participant_id))

# run 2pl model
model.2pl =
  # predict by difficulty + ability
  ltm(df.irt_wide ~ z1,
      # print IRT parameters (coefficient estimates of item difficulty and discrimination)
      IRT.param = TRUE,
      # 1000 iterations to yield estimates
      control = list(iter.em = 1000))

# item characteristic curves
plot(model.2pl,
     type = "ICC", zrange = c(-4, 4))

# item information plot
plot(model.2pl,
     type = "IIC", zrange = c(-4, 4))

# Wright map
fit_2pl_tam =
  tam.mml.2pl(df.irt_wide)

IRT.WrightMap(fit_2pl_tam)

```

#### BRBF

The algorithm did not converge.

```{r warning = F, message = F, results='hide'}

# Run for BRBF
df.irt_wide = 
  df.irt %>%
  # filter for BRBF items
  filter(test_name == "BRBF") %>%
  dplyr::select(participant_id, item_id, correct_response) %>%
  pivot_wider(names_from = item_id,
              values_from = correct_response) %>%
  dplyr::select(-c(participant_id))

# run 2pl model
model.2pl =
  # predict by difficulty + ability
  ltm(df.irt_wide ~ z1,
      # print IRT parameters (coefficient estimates of item difficulty and discrimination)
      IRT.param = TRUE,
      # 1000 iterations to yield estimates
      control = list(iter.em = 1000))

# item characteristic curves
plot(model.2pl,
     type = "ICC", zrange = c(-4, 4))

# item information plot
plot(model.2pl,
     type = "IIC", zrange = c(-4, 4))

# Wright map
fit_2pl_tam =
  tam.mml.2pl(df.irt_wide)

IRT.WrightMap(fit_2pl_tam)

```

#### VLAT

In the plots below, we see that for many items, respondents low on data visualization
literacy still have a decent probability of choosing the correct answer. Items differ
quite widely in their discrimination. For some items, item difficulty is high (even with
high levels of ability, respondents have a low estimated probability of selecting the
correct answer.

```{r warning = F, message = F, results='hide'}

# Run for VLAT
df.irt_wide = 
  df.irt %>%
  # filter for VLAT items
  filter(test_name == "VLAT") %>%
  dplyr::select(participant_id, item_id, correct_response) %>%
  pivot_wider(names_from = item_id,
              values_from = correct_response) %>%
  dplyr::select(-c(participant_id))

# run 2pl model
model.2pl =
  # predict by difficulty + ability
  ltm(df.irt_wide ~ z1,
      # print IRT parameters (coefficient estimates of item difficulty and discrimination)
      IRT.param = TRUE,
      # 1000 iterations to yield estimates
      control = list(iter.em = 1000))

# item characteristic curves
plot(model.2pl,
     type = "ICC", zrange = c(-4, 4))

# item information plot
plot(model.2pl,
     type = "IIC", zrange = c(-4, 4))

# Wright map
fit_2pl_tam =
  tam.mml.2pl(df.irt_wide)

IRT.WrightMap(fit_2pl_tam)

```

#### CALVI

In the plots below, we see that for many items, IICs seem uninterpretable; as ability
level increases, respondents have a lower probability of selecting the correct answer.

```{r warning = F, message = F, results='hide'}

# Run for CALVI
df.irt_wide = 
  df.irt %>%
  # filter for CALVI
  filter(test_name == "CALVI") %>%
  dplyr::select(participant_id, item_id, correct_response) %>%
  pivot_wider(names_from = item_id,
              values_from = correct_response) %>%
  dplyr::select(-c(participant_id))

# run 2pl model
model.2pl =
  # predict by difficulty + ability
  ltm(df.irt_wide ~ z1,
      # print IRT parameters (coefficient estimates of item difficulty and discrimination)
      IRT.param = TRUE,
      # 1000 iterations to yield estimates
      control = list(iter.em = 1000))

# item characteristic curves
plot(model.2pl,
     type = "ICC", zrange = c(-4, 4))

# item information plot
plot(model.2pl,
     type = "IIC", zrange = c(-4, 4))

# Wright map
fit_2pl_tam =
  tam.mml.2pl(df.irt_wide)

IRT.WrightMap(fit_2pl_tam)

```

## RQ4: How well does a model simulating a low-literacy, guessing participant perform?

The item discrimination and item information analyses suggest that even students with
relatively low data visualization literacy have a non-negligible chance of getting the
correct response on various items.

Next, we run a model representing a student with low data visualization literacy who
"guesses" the answer to a given item in a semi-informed manner (e.g., using an exclusion
strategy or similar).

### Data Wrangling

```{r}

# define dataset
df.irt_wide = 
  df.irt %>%
  # keep only items and response values
  dplyr::select(participant_id, item_id, correct_response) %>%
  # create matrix containing only 1s and 0s for correct/incorrect responses
  pivot_wider(names_from = item_id,
              values_from = correct_response) %>%
  # remove participant id
  dplyr::select(-c(participant_id))

```

```{r}

# create df with column representing binary vs MC nature of each item
df.binaryvsmc =
  df.irt %>% 
  dplyr::select(item_id, possible_responses, correct_response) %>% 
  distinct() %>%
  # count the response options for each item
  mutate(n_options = str_count(possible_responses, ",") + 1,
         item_type = if_else(n_options == 2, "binary", "mc"))

# obtain lists of item ids for binary and mc items
# binary items
binary_items =
  df.binaryvsmc %>%
  filter(item_type == "binary") %>%
  pull(item_id)

# mc items
mc_items =
  df.binaryvsmc %>%
  filter(item_type == "mc") %>%
  pull(item_id)

```

### Modeling

#### Binary Items: IRT-based model

For binary items, we use a 2PL model where we set the latent ability (or theta) to \~1 SD
below the average to simulate a low-literacy test-taker.

For any item whose difficulty is around â€“1, the 2-PL yields about a coin-flip chance of
obtaining the correct response.

$$P(\text{correct}\mid\theta=-1) \;
=\; \text{logit}^{-1}\!\bigl(a(\theta-b)\bigr)
\approx 0.50.$$

For easier items (difficulty \< â€“1) the model lets the learner do better than chance, and
for harder items (b \> â€“1) it lets them do worse.

```{r results='hide'}

# fit 2pl model to generate predictions for binary items
model.2pl =
  # predict by difficulty + ability
  ltm(df.irt_wide ~ z1,
      # print IRT parameters (coefficient estimates of item difficulty and discrimination)
      IRT.param = TRUE,
      # 1000 iterations to yield estimates
      control = list(iter.em = 1000))

# pass coefficients from model as parameters
pars = coef(model.2pl)

# define logistic function, where a = discrimination and b = difficulty
logistic =
  function(theta, a, b) 1 / (1 + exp(-a * (theta - b)))

# define data viz literacy as 1 SD below the mean (theta = -1)
theta = -1

# obtain discrimination and difficulty for binary items
P_binary =
  logistic(theta,
           pars[binary_items, "Dscrmn"],
           pars[binary_items, "Dffclt"])

# generate predictions for each item
pred_binary =
  tibble(
    item_id = names(P_binary),
    proportion_correct = round(100 * P_binary, 2)
    )

```

#### MC Items: Exclusion-strategy guesser

For multiple-choice items, we assume that the student follows an exclusion strategy: They
can rule out a non-negligible number of options at a coin-flip rate, and then guess among
the remaining options.

```{r}

# predictions for MC items

# define parameters
theta = .5 # chance the simulated student eliminates a given response option
elim_prop  = .75 # student eliminates max 75 % of wrong options
n_sim = 426 # we simulate N = 426 respondents to match our data

# define function to get prediction for each MC item
get_mc_pred =
  function(opts_string, n_sim, theta, elim_prop) {
    # count the number of response options for each item
    n_options = str_count(opts_string, ",") + 1
    # define the number of wrong options as number of options minus 1
    k_wrong = n_options - 1
    if (k_wrong == 0) return(1)
    # set a dynamic maximum of eliminated options (â‰¤ 75% of options)
    max_elim = floor(elim_prop * k_wrong)
    # set chance the student successfully eliminates any single wrong option based on parameters above
    n_elim = pmin(rbinom(n_sim, k_wrong, theta), max_elim)
    # define the remaining wrong options as the number of wrong options minus eliminated options
    remain_wrong = k_wrong - n_elim
    # calculate expected probability of dplyr::selecting correct response
    mean(1 / (remain_wrong + 1))
  }

# generate predictions for each mc item
pred_mc =
  df.binaryvsmc %>% 
  # filter for mc items
  filter(item_type == "mc") %>% 
  distinct(item_id, possible_responses) %>%
  mutate(
    proportion_correct = round(
      100 * map_dbl(
        possible_responses,
        # run prediction function using three parameters
        ~ get_mc_pred(.x,
                      n_sim = n_sim,
                      theta = theta,
                      elim_prop = elim_prop)
        ), 2) # round to two digits
  ) %>% 
  # keep only relevant cols
  dplyr::select(item_id, proportion_correct)

```

```{r}

# merge datasets
prediction_final =
  bind_rows(
    pred_binary %>%
      mutate(item_id = as.numeric(item_id)),
    pred_mc %>%
      mutate(item_id = as.numeric(item_id))
    ) %>% 
  arrange(item_id)

```

```{r}

# create dataset with observed % correct values per item
observed_final =
  df.irt %>%
  group_by(item_id) %>% 
  summarise(actual_prop = 100 * mean(correct_response, na.rm = TRUE),
            test_name   = first(test_name),
            .groups = "drop") %>% 
  mutate(item_id = as.numeric(item_id))

# create new dataset merging observed proportions with predicted proportions
results =
  prediction_final %>% 
  rename(predicted_prop = proportion_correct) %>% 
  left_join(observed_final, by = "item_id")

```

#### Viz 5: Predicted vs Observed Item Difficulty

```{r fig.width = 10, fig.height = 6}

correlation = cor.test(results$predicted_prop, results$actual_prop)

# plot
results %>%
  ggplot(aes(x = predicted_prop,
             y = actual_prop,
             color = test_name)) +
  geom_point(alpha = 0.5, size = 2) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed",
              color = "darkblue", size = 1) +
  scale_color_brewer(palette = "Set1", name = "Test") +
  annotate("text", x = 100, y = 5,
         label = paste0("r = ", round(correlation$estimate, 3)),
         hjust = 1, vjust = 0,
         size = 7, fontface = "bold") +
  labs(
    title  = "Predicted vs. Observed Item Difficulty (measured as % correct)",
    subtitle = "Individual dots represent individual test items",
    x = "Predicted % correct ('low-literacy guessing student' model)",
    y = "Observed % correct (N = 426 subjects)"
  )

# export plot
ggsave((paste0(figures_dir,
               "guessing_model.png")),
       width = 10, height = 6, device = "png")

```

#### Model Statistics

The correlation analysis shows that the model predictions are significantly but only
weakly correlated with the observed data. The RMSE evaluation shows that the low-literacy
guessing model performs worse than a baseline model that fits just the mean. In other
words, the guessing model is not a good explanation for the data at hand; it is highly
unlikely that high-performing respondents achieved a large number of correct responses by
simply doing (informed) guessing.

*r*(200) = .22, 95% CIs = [.09, .35], *p* = .002

*RMSE_model* = 35.17 vs *RMSE_baseline **=*** 27.22 (scale: whole percentage points,
0-100)

```{r}

# Simple correlation
correlation = cor.test(results$predicted_prop, results$actual_prop)
print(correlation)

# Calculate RMSE
# RMSE of fitting the mean
baseline_rmse =
  sqrt(mean((results$actual_prop - mean(results$actual_prop))^2))

print(baseline_rmse)

# RMSE of model
rmse =
  sqrt(mean((results$predicted_prop - results$actual_prop)^2,
            na.rm = TRUE))

print(rmse)

cat("Baseline RMSE =", round(baseline_rmse, 1),"%\n",
    "Model   RMSE =", round(rmse, 1),"%\n")

```

# Predicting the Held-Out Data

We have a held-out 20% of data that contains entirely new items. We now have the models
above make predictions for that held-out dataset (predicting proportion correct for these
new items).

## Data Wrangling

```{r}

# Import held out data with participant info
df.heldout =
  read.csv(paste0(data_dir, "test_data.csv"))

# add column with binary vs MC information
df.heldout_binaryvsmc =
  df.heldout %>% 
  dplyr::select(item_id, possible_responses) %>% 
  distinct() %>%
  # count the response options for each item
  mutate(n_options = str_count(possible_responses, ",") + 1,
         item_type = if_else(n_options == 2, "binary", "mc"))

# pull IDs for binary items
binary_items =
  df.heldout_binaryvsmc %>%
  filter(item_type == "binary") %>%
  pull(item_id)

# pull IDs for mc items
mc_items =
  df.heldout_binaryvsmc %>%
  filter(item_type == "mc") %>%
  pull(item_id)

```

## Modeling

### Binary Items: IRT-based model

```{r}

# run predictions for binary items
theta = -1

P_binary =
  logistic(theta,
           pars[binary_items, "Dscrmn"],
           pars[binary_items, "Dffclt"])

pred_binary_heldout =
  tibble(
    item_id = binary_items,
    predicted_prop = round(100 * P_binary, 2)
)

```

### MC Items: Exclusion-strategy guesser

```{r}

# run predictions for MC items
# define parameters
theta = .5 # chance the simulated student eliminates a given response option
elim_prop  = .75 # student eliminates max 75 % of wrong options
n_sim = 426 # we simulate N = 426 respondents to match our data

# define function to get prediction for each MC item
get_mc_pred =
  function(opts_string, n_sim, theta, elim_prop) {
    # count the number of response options for each item
    n_options = str_count(opts_string, ",") + 1
    # define the number of wrong options as number of options minus 1
    k_wrong = n_options - 1
    if (k_wrong == 0) return(1)
    # set a dynamic maximum of eliminated options (â‰¤ 75% of options)
    max_elim = floor(elim_prop * k_wrong)
    # set chance the student successfully eliminates any single wrong option based on parameters above
    n_elim = pmin(rbinom(n_sim, k_wrong, theta), max_elim)
    # define the remaining wrong options as the number of wrong options minus eliminated options
    remain_wrong = k_wrong - n_elim
    # calculate expected probability of dplyr::selecting correct response
    mean(1 / (remain_wrong + 1))
  }

# generate predictions for mc items
pred_mc_heldout =
  df.heldout_binaryvsmc %>% 
  filter(item_type == "mc") %>% 
  distinct(item_id, possible_responses) %>%
  mutate(
    predicted_prop = round(
      100 * map_dbl(
        possible_responses,
        # run prediction function using three parameters
        ~ get_mc_pred(.x,
                      n_sim = n_sim,
                      theta = theta,
                      elim_prop = elim_prop)
        ), 2) # round to two digits
  ) %>% 
  # keep only relevant cols
  dplyr::select(item_id, predicted_prop)

```

```{r}

# merge datasets
export =
  bind_rows(
    pred_binary_heldout,
    pred_mc_heldout) %>% 
  mutate(predicted_prop = predicted_prop / 100) %>%
  rename(proportion_correct = predicted_prop) %>%
  arrange(item_id)

# export predictions
readr::write_csv(export, paste0(data_dir, "predictions.csv"))

```

### Viz 6: Predicted vs Observed Item Difficulty

```{r fig.width = 12, fig.height = 6}

results_heldout =
  df.heldout %>%
  dplyr::select(item_id, proportion_correct, test_name) %>%
  rename(actual_prop = proportion_correct) %>%
  left_join(export) %>%
  rename(predicted_prop = proportion_correct)

# plot
results_heldout %>%
  ggplot(aes(x = (predicted_prop*100),
             y = (actual_prop*100),
             color = test_name)) +
  geom_point(alpha = 0.5, size = 2) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed",
              color = "darkblue", size = 1) +
  scale_color_brewer(palette = "Set1", name = "Test") +
  ylim(0, 100) +
  xlim(0, 100) +
  labs(
    title  = "HELD-OUT DATA: Predicted vs. Observed Item Difficulty (measured as % correct)",
    subtitle = "Individual dots represent individual test items",
    x = "Predicted % correct ('low-literacy guessing student' model)",
    y = "Observed % correct (N = 426 subjects)"
  )

# export plot
ggsave((paste0(figures_dir,
               "guessing_model_heldout.png")),
       width = 12, height = 6, device = "png")

```
