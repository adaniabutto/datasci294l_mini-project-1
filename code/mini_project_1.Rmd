---
title: "DATSCI294L: Data Science and the Science of Learning"
subtitle: "Mini-Project #1: Predicting the diffi culty of a test item"
author: "Adani B. Abutto"
date: "`r format(Sys.Date(), '%B %d, %Y')`"

output:
  bookdown::html_document2:
    toc: true
    toc_depth: 3
    toc_float: true
    theme: cosmo
    highlight: tango

header-includes:
    - \usepackage{setspace}\doublespacing
---

# Setup

```{r, message = F, warning = F}

## load relevant libraries and functions
require(knitr)         # for knitting
library(Hmisc)         # for descriptives
library(png)           # for working with images
library(grid)
library(DT)            # for data tables
library(tidyverse)     # for everything else
library(ggeffects)     # for regression outputs
library(emmeans)
library(generics)
library(broom)
library(modelr)
library(ltm)           # for IRT stuff
library(TAM)
library(psych)

## set default code chunk options
knitr::opts_chunk$set(echo = T, warning = F, message = F)

## set default plot theme and colors
theme_set(theme_classic(base_size = 18))

## fix print width for knitted doc
options(width = 70)

## suppress warnings about grouping 
options(dplyr.summarise.inform = F)
options(xtable.floating = F)
options(xtable.timestamp = "")

## set random seed
set.seed(1)

## set directories for plots and data outputs
figures_dir = '../figures/'
data_dir = '../data/'

```

# Phase 1: Exploring and visualizing the data

## Load and examine the data

```{r}

# Import data as downloaded from https://data-visualization-benchmark.s3.us-west-2.amazonaws.com/vt-fusion/psych_139_mini_project_1_split_80_responses.csv

df.raw =
  read.csv(paste0(data_dir,
                  "psych_139_mini_project_1_split_80_responses.csv"))

# run quick data summary
skimr::skim(df.raw)

```

## Data Wrangling

```{r}

# convert data to wide format so each row represents one participant (N = 426)

df.wide =
  df.raw %>%
  # grab response values for each question from "correct_response" col
  pivot_wider(id_cols = "participant_id",
              names_from = c("question_text"),
              values_from = c("correct_response"),
              values_fn = mean)

# print top rows
head(df.wide, n = 10)

```

## Preliminary Item Difficulty Analysis: % correct

Across all tests, we have a total of 184 questions, with about 80-90 responses per
question.

```{r}

# Create summary df: For each question, compute % of participants who responded correctly (mean), SD, and SE

df.wide_summary =
  df.raw %>%
  group_by(question_text, test_name, graph_type, task_type_merged) %>%
  # compute mean
  dplyr::summarise(mean_correct = mean(correct_response,
                                       na.rm = T),
                   # compute SD
                   sd_correct = sd(correct_response,
                                   na.rm = T),
                   # compute number of responses
                   n = n(),
                   # compute SE (SD / sqrt of n)
                   (1.96*se_correct) = sd_correct/sqrt(n),) %>%
  ungroup()

# print top rows
head(df.wide_summary, n = 10)

```

### Viz 1: % correct for single items

The plot below shows that as expected, even within a single test, the items varied
considerably in their difficulty (as measured by % correct).

```{r fig.width = 20, fig.height = 10}

df.wide_summary %>%
  # question on x-axis, ordered by % correct
  ggplot(aes(x = reorder(question_text, mean_correct),
             # % correct on y-axis
             y = mean_correct*100),
         # color each bar based on graph type
         color = graph_type) +
  # add CIs based on 1.96*SE
  geom_errorbar(
    aes(ymin = (mean_correct - (1.96*se_correct))*100,
        ymax = (mean_correct + (1.96*se_correct))*100),
    width = 2,
    color = "black",
    alpha = .7,
    size = 1) +
  # plot one point per question/item
  geom_point(aes(fill = graph_type),
             shape = 21, color = "black", size = 2) +
  # facet by task type and test
  facet_wrap(~ task_type_merged + test_name,
             ncol = 5, scales = "free_y") +
  # adjust x-axis label orientation for readability
  theme(axis.text.x = element_text(angle = 45,
                                   hjust = 1,
                                   size = 1),
        legend.position = "none") +
  # fix scale to be 0-100, breaks of 25
  scale_y_continuous(limits = c(0, 100),
                     breaks = seq(0, 100, 25)) +
  # add title and axis labels
  labs(title = "Individual Item Difficulty by Task Type & Test",
       x = "Question",
       y = "% correct")

# export plot
ggsave((paste0(figures_dir,
               "pct_correct_tasktype_test.png")),
       width = 20, height = 10, device = "png")

```

### Viz 2: % correct averages by task type

The plot below shows that even though different tests contained items of the same "task
type", their respective item difficulty still varied across tests.

```{r}

# create new sub-dataframe: Within a given test, collapse across a given task type and compute % of participants who responded correctly (mean), SD, and SE
df.task_summary_tasktype =
  df.raw %>%
  # group by test and task type
  group_by(test_name, task_type_merged) %>%
  # compute mean
  summarise(mean_correct = mean(correct_response, na.rm = T),
            # compute SD
            sd_correct = sd(correct_response, na.rm = T),
            # compute number of responses
            n = n(),
            # compute SE
            (1.96*se_correct) = sd_correct / sqrt(n))

```

```{r fig.width = 10, fig.height = 6}

df.task_summary_tasktype %>%
  # test type on x-axis
  ggplot(aes(x = test_name,
             # % correct on y-axis
             y = (mean_correct*100),
             # color by test name
             fill = test_name,
             color = test_name)) +
  # add CIs based on 1.96*SE
  geom_errorbar(aes(ymin = (mean_correct - (1.96*(1.96*se_correct)))*100,
              ymax = (mean_correct + (1.96*se_correct))*100),
          position = position_dodge(width = 0.9),
          width = 0.1,
          color = "black") +
  # Add one point for each % correct collapsed across task type
  geom_point(position = position_dodge(width = 0.9),
             width = 1,
             size = 3) +
  # Adjust scales to be 0-100
  scale_y_continuous(limits = c(0, 100),
                     breaks = seq(0, 100, 25),
                     labels = scales::percent_format(scale = 1)) +
  # Add title and axis labels
  labs(title = "% correct averaged across Task Type",
       x = "Task Type",
       y = "% correct",
       fill = "Test") +
  # Adjust theme
  theme_minimal(base_size = 18) +
  theme(axis.text.x = element_blank(),
        axis.title.x = element_blank(),
        legend.position = "bottom") +
  guides(fill = "none") +
  # Facet by task type
  facet_grid(~task_type_merged)

# export plot
ggsave((paste0(figures_dir,
               "pct_correct_tasktype.png")),
       width = 10, height = 6, device = "png")

```

### Descriptives: Single-item descriptives for % correct

In the output below, we see that on some items, as few as 1% of participants selected the
correct response (very difficult items), and on other items, as many as 99% of
participants selected the correct response (very easy items).

Across items, SDs of % correct ranged from 10.6% to 50%.

```{r}

## Across dataset, compute means and SDs as well as mins and maxes for each respective statistic
df.wide_summary %>%
  dplyr::select(mean_correct, sd_correct) %>%
  summary()

# Print 10 questions with lowest average % correct
df.wide_summary %>%
  dplyr::select(question_text, mean_correct, sd_correct) %>%
  arrange(desc(mean_correct)) %>%
  tail(n = 10) %>%
  print()

# Print 10 questions with highest average % correct
df.wide_summary %>%
  dplyr::select(question_text, mean_correct, sd_correct) %>%
  arrange(desc(mean_correct)) %>%
  head(n = 10) %>%
  print()

# Print 10 questions with least variability
df.wide_summary %>%
  dplyr::select(question_text, mean_correct, sd_correct) %>%
  arrange(desc(sd_correct)) %>%
  tail(n = 10) %>%
  print()

# Print 10 questions with most variability
df.wide_summary %>%
  dplyr::select(question_text, mean_correct, sd_correct) %>%
  arrange(desc(sd_correct)) %>%
  head(n = 10) %>%
  print()

```

## Phase 2: Defining & evaluating statistical models

In the dataset we examined above, we have data from a series of tests that (supposedly)
tap data viz literacy. Notably, not all items in and across these tests are the same. In
any test, you want a variety of items. Based on this variety, it’s likely some items will
be harder (and some will be easier) than others. I begin by performing some sanity checks
that assess whether the items we would expect to be harder were indeed harder for
participants (as per lower % correct).

### RQ1: Are items that were *designed to mislead* harder than those that were not?

That is, does misleading nature (categorical; yes vs. no) predict % correct?

**Note:** Only CALVI included misleading items. Thus, we examine data only from this test
(4,082 responses from *N* = 425 participants).

**Prediction:** Misleading items are lower % correct (i.e., higher difficulty) than
non-misleading items.

**Method:** Simple regression: $y = b_0 + x*b_1$, where $y$ = % correct and $x$ =
misleading nature (0 or 1).

```{r}

# Extract relevant subset of data
df.misleading =
  df.raw %>%
  # Keep only relevant cols
  dplyr::select(participant_id, question_text, test_name, graph_type, task_type_merged, correct_response, misleading_item) %>%
  # Filter for CALVI data
  filter(test_name == "CALVI") %>%
  # Convert "misleading nature" into factor (True vs false)
  mutate(misleading_item = factor(misleading_item))

```

Out of the 48 items we have data on in CALVI, 36 are misleading (75%), and 12 are not
(25%).

```{r}

# count misleading items
df.misleading %>%
  distinct(question_text, misleading_item) %>%
  count(misleading_item)

```

#### Descriptives: Mean and SD for misleading vs. non-misleading items

```{r}

df.misleading %>%
  group_by(misleading_item) %>%
  summarise(mean = mean(correct_response, na.rm = T),
            sd = sd(correct_response, na.rm = T),
            n = n())

```

#### Viz 3: % correct for misleading vs non-misleading items

In line with the descriptive stats above, the plot shows that at first glance, the average
% correct for misleading items seems to be lower than the average % correct for
non-misleading items.

```{r fig.width = 16, fig.height = 8}

# Plot the raw data
df.misleading %>%
  # misleading nature (true vs false) on x-axis
  ggplot(aes(x = misleading_item,
             # % correct on y-axis
             y = (correct_response*100),
             # color by misleading nature
             fill = misleading_item)) +
  # add raw data points
  geom_point(position = position_jitter(width = .3,
                                        height = 0),
             shape = 21,
             size = 2,
             alpha = .3) +
  # add bootstrapped CIs for error bars
  stat_summary(fun.data = "mean_cl_boot",
               geom = "pointrange",
               color = "black",
               shape = 21,
               size = 1,
               linewidth = 3) +
  # adjuts scales
  scale_y_continuous(limits = c(0, 100),
                     breaks = seq(0, 100, 25),
                     labels = scales::percent_format(scale = 1)) +
  # add color scale for fill
  scale_fill_brewer(palette = "Pastel2") +
  # Adjust title and axis labels
  labs(title = "% correct by misleading vs. non-misleading items",
       x = "Misleading Item",
       y = "% correct",
       fill = "Misleading?") +
  # Adjust theme
  theme_classic(base_size = 36) +
  theme(legend.position = "none")

# export plot
ggsave((paste0(figures_dir,
               "pct_correct_misleading.png")),
       width = 16, height = 8, device = "png")
  
```

#### Regression Analysis: % correct \~ misleading

The regression analysis shows that compared to a minimal model that fits the mean
regardless of the difference in the nature of the items (misleading vs not misleading),
the model including misleading nature as a predictor of % correct reduces the error by
\~10%. Misleading nature is a weak predictor.

*F*(1, 4080) = 431.54, p \< .001, $R^2$ = .095.

*RMSE_model* = 0.472 vs *RMSE_baseline **=** .*496.

```{r}

# Run empty model fitting the mean
model.misleading_empty =
  lm(correct_response ~ 1,
      data = df.misleading)

model.misleading_empty %>%
  summary()

# RMSE
rmse_empty =
  model.misleading_empty %>% 
  rmse(data = df.misleading)

print(rmse_empty)

########################

# Run augmented model adding predictor of interest (misleading nature)
model.misleading_augmented =
  lm(correct_response ~ 1 + misleading_item,
      data = df.misleading)

# Print regression output
model.misleading_augmented %>%
  summary()

model.misleading_augmented %>%
  joint_tests()

# Show estimates of augmented model
model.misleading_augmented %>%
  ggpredict()

# Calculate model statistics
model.misleading_augmented %>% 
  glance()

# Compute RMSE
rmse_misleading =
  model.misleading_augmented %>% 
  rmse(data = df.misleading)

print(rmse_misleading)

# Compare empty model with augmented model
anova(model.misleading_empty, model.misleading_augmented)

```

#### Model Visualizations

```{r fig.width = 16, fig.height = 8}

# plot the data with line for empty model prediction (mean)
df.misleading %>%
  # x-axis is misleading nature
  ggplot(aes(x = misleading_item,
             # y-axis is % correct
             y = (correct_response*100),
             # color by misleading nature
             fill = misleading_item)) +
  # add raw data points
  geom_point(position = position_jitter(width = .3,
                                        height = 0),
             shape = 21,
             size = 2,
             alpha = .3) +
  # add bootstrapped CIs
  stat_summary(fun.data = "mean_cl_boot",
               geom = "pointrange",
               color = "black",
               shape = 21,
               size = 1,
               linewidth = 3) +
  # add horizontal line for mean
  geom_hline(yintercept = mean(df.misleading$correct_response)*100,
             color = "lightblue", size = 3) +
  # adjust scale
  scale_y_continuous(limits = c(0, 100),
                     breaks = seq(0, 100, 25),
                     labels = scales::percent_format(scale = 1)) +
  # specify color fill
  scale_fill_brewer(palette = "Pastel2") +
  # add title and axis labels
  labs(title = "% correct by misleading vs. non-misleading items",
       x = "Misleading Item",
       y = "% correct",
       fill = "Misleading?") +
  # adjust theme
  theme_classic(base_size = 36) +
  theme(legend.position = "none")

```

```{r fig.width = 16, fig.height = 8}

# plot the data with lines for augmented model predictions (separate fitted means for misleading and non-misleading items)
df.misleading %>%
  # x-axis = misleading nature
  ggplot(aes(x = misleading_item,
             # y-axis = % correct
             y = (correct_response*100),
             # fill by misleading nature
             fill = misleading_item)) +
  # add raw data points
  geom_point(position = position_jitter(width = .3,
                                        height = 0),
             shape = 21,
             size = 2,
             alpha = .3) +
  # add boostrapped CIs
  stat_summary(fun.data = "mean_cl_boot",
               geom = "pointrange",
               color = "black",
               shape = 21,
               size = 1,
               linewidth = 3) +
  # add separate fitted lines for each mean
  geom_hline(yintercept = mean(df.misleading$correct_response[df.misleading$misleading_item == "True"])*100,
             color = "orange", size = 3) +
    geom_hline(yintercept = mean(df.misleading$correct_response[df.misleading$misleading_item == "False"])*100,
           color = "lightgreen", size = 3) +
  # adjust scale
  scale_y_continuous(limits = c(0, 100),
                     breaks = seq(0, 100, 25),
                     labels = scales::percent_format(scale = 1)) +
  # specify color fill
  scale_fill_brewer(palette = "Pastel2") +
  # add title and axis labels
  labs(title = "% correct by misleading vs. non-misleading items",
       x = "Misleading Item",
       y = "% correct",
       fill = "Misleading?") +
  # adjust  theme
  theme_classic(base_size = 36) +
  theme(legend.position = "none")

```

### RQ2: Are longer questions harder than shorter questions?

That is, does question length (continuous) predict % correct?

**Note:** I examined data from all tests (15,616 responses from *N* = 426 participants and
defined question length as the number of characters in the question.

**Prediction:** Longer questions are lower % correct (i.e., higher difficulty) than short
questions.

**Method:** Simple regression: $y = b_0 + x*b_1$, where $y$ = % correct and $x$ = question
length.

```{r}

# Extract relevant subset of data
df.length =
  df.raw %>%
  # Keep only relevant cols
  dplyr::select(participant_id, test_name, graph_type, task_type_merged, correct_response, question_text) %>%
  # Compute question length
  mutate(question_length = nchar(question_text))

# create summary df
df.length_summary =
  df.length %>%
  dplyr::select(question_text, correct_response) %>%
  group_by(question_text) %>%
  summarise(
    percent_correct = mean(correct_response, na.rm = TRUE) * 100,
    n = n(),
    question_length = nchar(first(question_text))
  ) %>%
  ungroup()

```

#### Descriptives: Mean and SD of question length

There is some variation in question length both within and across tests, with the average
question length ranging from 48 to 95 characters.

```{r}

# create summary statistics for question length variable
df.length %>%
  group_by(test_name) %>%
  summarise(mean = mean(question_length),
            sd = sd(question_length))

```

#### Viz 4: % correct by question length

The plot shows that at first glance, most questions are clustered around question length
50-150. Even within that range, % correct is highly variable, and there is not much of a
visually clear trend.

```{r}

# Plot raw data
df.length_summary %>%
  # x-axis = question length
  ggplot(aes(x = question_length,
             # y-axis = % correct
             y = percent_correct)) +
  # plot raw data
  geom_point(position = position_jitter(width = .5,
                                        height = 0),
             size = 2,
             alpha = .3) +
  # adjust scale
  scale_y_continuous(limits = c(0, 100),
                     breaks = seq(0, 100, 25),
                     labels = scales::percent_format(scale = 1)) +
  xlim(0, 375) +
  # add axis labels
  labs(y = "% correct",
       x = "Question Length")

```

#### Regression Analysis: % correct \~ question length

The regression analysis shows that compared to a minimal model that fits the mean
regardless of the difference in question length, the model including question length as a
predictor of % correct reduces the error by \~4%. Question length is a weak predictor.

*F*(1, 148) = 6.992, *p* = .009, $R^2$ = .039.

*RMSE_model* = 26.84 vs *RMSE_baseline **=*** 27.46.

```{r}

# Run empty model fitting the mean
model.length_empty =
  lm(percent_correct ~ 1,
      data = df.length_summary)

model.length_empty %>%
  summary()

# RMSE
rmse_empty =
  model.length_empty %>% 
  rmse(data = df.length_summary)

print(rmse_empty)

########################

# Run augmented model adding predictor of interest (question length)
model.length_augmented =
  lm(percent_correct ~ 1 + question_length,
      data = df.length_summary)

# Print regression output
model.length_augmented %>%
  summary()

model.length_augmented %>%
  joint_tests()

# Calculate model statistics
model.length_augmented %>% 
  glance()

# Compute RMSE
rmse_questionlength =
  model.length_augmented %>% 
  rmse(data = df.length_summary)

print(rmse_questionlength)

# Compare empty model with augmented model
anova(model.length_empty, model.length_augmented)


```

#### Model Visualizations

```{r}

# plot empty model with mean
df.length_summary %>%
  # x-axis = question length
  ggplot(aes(x = question_length,
             # y-axis = % correct
             y = percent_correct)) +
  # plot raw data points
  geom_point(position = position_jitter(width = .5,
                                        height = 0),
             size = 2,
             alpha = .5) +
  # adjust scale
  scale_y_continuous(limits = c(0, 100),
                     breaks = seq(0, 100, 25),
                     labels = scales::percent_format(scale = 1)) +
  # add fitted line for mean
  geom_hline(yintercept = mean(df.length_summary$percent_correct),
             color = "lightblue", size = 1) +
  xlim(0, 375) +
  # adjust axis labels
  labs(y = "% correct",
       x = "Question Length")

```

```{r}

# plot the data with a fitted line for the augmented model predictions
df.length_summary %>%
  # question length on x-axis
  ggplot(aes(x = question_length,
             # % correct on y-axis
             y = percent_correct)) +
  # add raw data
  geom_point(alpha = 0.6, size = 2) +
  # add regression line
  geom_smooth(method = "lm", se = TRUE, color = "lightblue") +
  # adjuts scale
  scale_y_continuous(limits = c(0, 100),
                     breaks = seq(0, 100, 25),
                     labels = scales::percent_format(scale = 1)) +
  # add title and axis labels
  labs(title = "% correct by question length",
       x = "Question Length",
       y = "% correct")

```

### RQ3: How informative and discriminative are the items in relation to data visualization literacy?

#### Item Discrimination and Item Information

```{r}

# create new df
df.irt =
  df.raw

```

Next, we evaluate item information and item discrimination for all test items across all
tests from an IRT perspective.

In IRT, item difficulty is defined as the level of latent ability (i.e., data
visualization) or point on the ability scale (represented by theta) needed to have a 50%
probability of getting the item right. Harder items require more ability to have a 50%
probability of getting the item right. Easier items require less ability to have a 50%
probability of getting the item right.

We fit two-parameter logistic (2PL) models, where the expected difference in performance
(i.e., probability of getting the item right) depends not just on ability level but also
the specific item in question.

**Item information** represents each item's ability to differentiate between test-takers
based on ability level (theta). We plot item information using Item Characteristic Curves
(ICCs), where the x-axis represents ability level (theta), and the y-axis represents
probability of getting a correct response. Higher item information is better.

**Item discrimination** represents how strongly related a given item is to the latent
ability in question as assessed by the broader test. Items with higher discrimination are
better at differentiating test-takers. More formally, it is the slope of a tangent at the
point of inflection of the item response function (IRF), or the point on the logistic
curve at which the curvature changes signs.

##### WAN

```{r}

# Run for WAN
df.irt_wide = 
  df.irt %>%
  filter(test_name == "WAN") %>%
  dplyr::select(participant_id, item_id, correct_response) %>%
  pivot_wider(names_from = item_id,
              values_from = correct_response) %>%
  dplyr::select(-c(participant_id))

model.2pl =
  ltm(df.irt_wide ~ z1,
      IRT.param = TRUE, control = list(iter.em = 1000))

# item characteristic curve
plot(model.2pl,
     type = "ICC", zrange = c(-4, 4))

# item information curve
plot(model.2pl,
     type = "IIC", zrange = c(-4, 4))

# Wright map
fit_2pl_tam =
  tam.mml.2pl(df.irt_wide)

IRT.WrightMap(fit_2pl_tam)

```

##### GGR

```{r}

# Run for GGR
df.irt_wide = 
  df.irt %>%
  filter(test_name == "GGR") %>%
  dplyr::select(participant_id, item_id, correct_response) %>%
  pivot_wider(names_from = item_id,
              values_from = correct_response) %>%
  dplyr::select(-c(participant_id))

# fit 2 parameter model
model.2pl =
  ltm(df.irt_wide ~ z1,
      IRT.param = TRUE, control = list(iter.em = 1000))

# item discrimination
plot(model.2pl,
     type = "ICC", zrange = c(-4, 4))

# item information
plot(model.2pl,
     type = "IIC", zrange = c(-4, 4))

# Wright map
fit_2pl_tam =
  tam.mml.2pl(df.irt_wide)

IRT.WrightMap(fit_2pl_tam)

```

### BRBF

```{r}

# Run for BRBF
df.irt_wide = 
  df.irt %>%
  filter(test_name == "BRBF") %>%
  dplyr::select(participant_id, item_id, correct_response) %>%
  pivot_wider(names_from = item_id,
              values_from = correct_response) %>%
  dplyr::select(-c(participant_id))

# fit 2 parameter model
model.2pl =
  ltm(df.irt_wide ~ z1,
      IRT.param = TRUE, control = list(iter.em = 1000))

# item discrimination
plot(model.2pl,
     type = "ICC", zrange = c(-4, 4))

# item information
plot(model.2pl,
     type = "IIC", zrange = c(-4, 4))

# Wright map
fit_2pl_tam =
  tam.mml.2pl(df.irt_wide)

IRT.WrightMap(fit_2pl_tam)

```

### VLAT

```{r}

# Run for VLAT
df.irt_wide = 
  df.irt %>%
  filter(test_name == "VLAT") %>%
  dplyr::select(participant_id, item_id, correct_response) %>%
  pivot_wider(names_from = item_id,
              values_from = correct_response) %>%
  dplyr::select(-c(participant_id))

# fit 2 parameter model
model.2pl =
  ltm(df.irt_wide ~ z1,
      IRT.param = TRUE, control = list(iter.em = 1000))

# item discrimination
plot(model.2pl,
     type = "ICC", zrange = c(-4, 4))

# item information
plot(model.2pl,
     type = "IIC", zrange = c(-4, 4))

# Wright map
fit_2pl_tam =
  tam.mml.2pl(df.irt_wide)

IRT.WrightMap(fit_2pl_tam)

```

### CALVI

```{r}

# Run for VLAT
df.irt_wide = 
  df.irt %>%
  filter(test_name == "CALVI") %>%
  dplyr::select(participant_id, item_id, correct_response) %>%
  pivot_wider(names_from = item_id,
              values_from = correct_response) %>%
  dplyr::select(-c(participant_id))

# fit 2 parameter model
model.2pl =
  ltm(df.irt_wide ~ z1,
      IRT.param = TRUE, control = list(iter.em = 1000))

# item discrimination
plot(model.2pl,
     type = "ICC", zrange = c(-4, 4))

# item information
plot(model.2pl,
     type = "IIC", zrange = c(-4, 4))

# Wright map
fit_2pl_tam =
  tam.mml.2pl(df.irt_wide)

IRT.WrightMap(fit_2pl_tam)

```

### RQ4: How well does a model simulating a low-literacy, guessing participant perform?

```{r}

# define dataset
df.irt_wide = 
  df.irt %>%
  dplyr::select(participant_id, item_id, correct_response) %>%
  pivot_wider(names_from = item_id,
              values_from = correct_response) %>%
  dplyr::select(-c(participant_id))

```

```{r}

# add column with binary vs MC information
df.binaryvsmc =
  df.irt %>% 
  dplyr::select(item_id, possible_responses, correct_response) %>% 
  distinct() %>%
  # count the response options for each item
  mutate(n_options = str_count(possible_responses, ",") + 1,
         item_type = if_else(n_options == 2, "binary", "mc"))

binary_items =
  df.binaryvsmc %>%
  filter(item_type == "binary") %>%
  pull(item_id)

mc_items =
  df.binaryvsmc %>%
  filter(item_type == "mc") %>%
  pull(item_id)

```

```{r}

# predictions for binary items
model.2pl =
  ltm(df.irt_wide ~ z1,
      IRT.param = TRUE, control = list(iter.em = 1000))

pars = coef(model.2pl)
logistic =
  function(theta, a, b) 1 / (1 + exp(-a * (theta - b)))

# define data viz literacy as 1 SD below the mean
theta = -1

# get discrimination and difficulty
P_binary =
  logistic(theta,
           pars[binary_items, "Dscrmn"],
           pars[binary_items, "Dffclt"])

# generate prediction for each item
pred_binary =
  tibble(
    item_id = names(P_binary),
    proportion_correct = round(100 * P_binary, 2)
    )

```

```{r}

# predictions for MC items

# define parameters
theta = .5 # chance the simulated student eliminates a given response option
elim_prop  = .75 # student eliminates max 75 % of wrong options
n_sim = 426 # we simulate N = 426 respondents

# define function to get prediction for each MC item
get_mc_pred =
  function(opts_string, n_sim, theta, elim_prop) {
    # count number of answer choices
    n_options = str_count(opts_string, ",") + 1
    # define number of wrong options as number of options minus 1
    k_wrong = n_options - 1
    if (k_wrong == 0) return(1)
    # set dynamic maximum of eliminated options (≥ 50% of options)
    max_elim = floor(elim_prop * k_wrong)
    # chance the student successfully eliminates any single wrong option based on parameters above
    n_elim = pmin(rbinom(n_sim, k_wrong, theta), max_elim)
    # remaining wrong options is number as wrong options minus eliminated options
    remain_wrong = k_wrong - n_elim
    # calculate expected probability of dplyr::selecting correct response
    mean(1 / (remain_wrong + 1))
  }

pred_mc =
  df.binaryvsmc %>% 
  filter(item_type == "mc") %>% 
  distinct(item_id, possible_responses) %>%
  mutate(
    proportion_correct = round(
      100 * map_dbl(
        possible_responses,
        ~ get_mc_pred(.x,
                      n_sim = n_sim,
                      theta = theta,
                      elim_prop = elim_prop)
        ), 2) # round to two digits
  ) %>% 
  dplyr::select(item_id, proportion_correct)

```

```{r}

# merge datasets
prediction_final =
  bind_rows(
    pred_binary %>%
      mutate(item_id = as.numeric(item_id)),
    pred_mc %>%
      mutate(item_id = as.numeric(item_id))
    ) %>% 
  arrange(item_id)

```

```{r}

# create dataset with observed % correct values per item
observed_final =
  df.irt %>%
  group_by(item_id) %>% 
  summarise(actual_prop = 100 * mean(correct_response, na.rm = TRUE),
            test_name   = first(test_name),
            .groups = "drop") %>% 
  mutate(item_id = as.numeric(item_id))

# create new dataset merging with predictions
results =
  prediction_final %>% 
  rename(predicted_prop = proportion_correct) %>% 
  left_join(observed_final, by = "item_id")

```

```{r fig.width = 10, fig.height = 6}

correlation = cor.test(results$predicted_prop, results$actual_prop)

# plot
results %>%
  ggplot(aes(x = predicted_prop,
             y = actual_prop,
             color = test_name)) +
  geom_point(alpha = 0.5, size = 2) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed",
              color = "darkblue", size = 1) +
  scale_color_brewer(palette = "Set1", name = "Test") +
  labs(
    title  = "Predicted vs. Observed Item Difficulty (measured as % correct)",
    subtitle = "Individual dots represent individual test items",
    x = "Predicted % correct ('low-literacy guessing student' model)",
    y = "Observed % correct (N = 426 subjects)"
  )

```

```{r}

# Calculate RMSE
# RMSE of fitting the mean
baseline_rmse =
  sqrt(mean((results$actual_prop - mean(results$actual_prop))^2))

print(baseline_rmse)

# RMSE of model
rmse =
  sqrt(mean((results$predicted_prop - results$actual_prop)^2,
            na.rm = TRUE))

print(rmse)

cat("Baseline RMSE =", round(baseline_rmse, 1),"%\n",
    "Model   RMSE =", round(rmse, 1),"%\n")

```

## Predicting the Held-Out Data

```{r}

# Import held out data with participant info
df.heldout =
  read.csv(paste0(data_dir, "test_data.csv"))

# add column with binary vs MC information
df.heldout_binaryvsmc =
  df.heldout %>% 
  dplyr::select(item_id, possible_responses) %>% 
  distinct() %>%
  # count the response options for each item
  mutate(n_options = str_count(possible_responses, ",") + 1,
         item_type = if_else(n_options == 2, "binary", "mc"))

binary_items =
  df.heldout_binaryvsmc %>%
  filter(item_type == "binary") %>%
  pull(item_id)

mc_items =
  df.heldout_binaryvsmc %>%
  filter(item_type == "mc") %>%
  pull(item_id)

```

```{r}

# run predictions for binary items
theta = -1

P_binary =
  logistic(theta,
           pars[binary_items, "Dscrmn"],
           pars[binary_items, "Dffclt"])

pred_binary_heldout =
  tibble(
    item_id = binary_items,
    predicted_prop = round(100 * P_binary, 2)
)

```

```{r}

# run predictions for MC items
# define parameters
theta = .5 # chance the simulated student eliminates a given response option
elim_prop  = .75 # student eliminates max 75 % of wrong options
n_sim = 426 # we simulate N = 426 respondents

# define function to get prediction for each MC item
get_mc_pred =
  function(opts_string, n_sim, theta, elim_prop) {
    # count number of answer choices
    n_options = str_count(opts_string, ",") + 1
    # define number of wrong options as number of options minus 1
    k_wrong = n_options - 1
    if (k_wrong == 0) return(1)
    # set dynamic maximum of eliminated options (≥ 50% of options)
    max_elim = floor(elim_prop * k_wrong)
    # chance the student successfully eliminates any single wrong option based on parameters above
    n_elim = pmin(rbinom(n_sim, k_wrong, theta), max_elim)
    # remaining wrong options is number as wrong options minus eliminated options
    remain_wrong = k_wrong - n_elim
    # calculate expected probability of dplyr::selecting correct response
    mean(1 / (remain_wrong + 1))
  }

pred_mc_heldout =
  df.heldout_binaryvsmc %>% 
  filter(item_type == "mc") %>% 
  distinct(item_id, possible_responses) %>%
  mutate(
    predicted_prop = round(
      100 * map_dbl(
        possible_responses,
        ~ get_mc_pred(.x,
                      n_sim = n_sim,
                      theta = theta,
                      elim_prop = elim_prop)
        ), 2) # round to two digits
  ) %>% 
  dplyr::select(item_id, predicted_prop)

```

```{r}

# merge datasets
export =
  bind_rows(
    pred_binary_heldout,
    pred_mc_heldout) %>% 
  mutate(predicted_prop = predicted_prop / 100) %>%
  rename(proportion_correct = predicted_prop) %>%
  arrange(item_id)

# export predictions
readr::write_csv(export, paste0(data_dir, "predictions.csv"))

```
